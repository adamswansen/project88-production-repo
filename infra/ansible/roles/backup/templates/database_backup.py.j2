#!/usr/bin/env python3
"""
Project88 Database Backup Script - {{ environment }} environment
Automated PostgreSQL backup to S3 with compression and encryption
"""

import os
import sys
import datetime
import subprocess
import logging
import boto3
from botocore.exceptions import ClientError, NoCredentialsError

# Configuration
ENVIRONMENT = "{{ environment }}"
S3_BUCKET = "{{ s3_bucket }}"
S3_PREFIX = f"{ENVIRONMENT}/daily"
POSTGRES_HOST = "{{ postgres_host }}"
POSTGRES_PORT = {{ postgres_port }}
POSTGRES_DB = "{{ postgres_db }}"
POSTGRES_USER = "{{ postgres_user }}"
BACKUP_DIR = "{{ backup_base_dir }}/temp"
LOG_FILE = "{{ backup_base_dir }}/logs/database_backup.log"

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def create_database_backup():
    """Create PostgreSQL database backup"""
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    backup_filename = f"project88_{ENVIRONMENT}_db_{timestamp}.sql"
    backup_path = os.path.join(BACKUP_DIR, backup_filename)
    
    try:
        # Set environment variables for pg_dump
        env = os.environ.copy()
        env['PGPASSWORD'] = "{{ vault_postgres_password }}"
        
        # Create pg_dump command
        cmd = [
            'pg_dump',
            '-h', POSTGRES_HOST,
            '-p', str(POSTGRES_PORT),
            '-U', POSTGRES_USER,
            '-d', POSTGRES_DB,
            '--no-password',
            '--verbose',
            '--clean',
            '--if-exists',
            '--create',
            '-f', backup_path
        ]
        
        logger.info(f"Starting database backup: {backup_filename}")
        result = subprocess.run(cmd, env=env, capture_output=True, text=True)
        
        if result.returncode != 0:
            logger.error(f"pg_dump failed: {result.stderr}")
            return None
            
        # Compress the backup
        compressed_path = f"{backup_path}.gz"
        compress_cmd = ['gzip', backup_path]
        subprocess.run(compress_cmd, check=True)
        
        logger.info(f"Database backup created: {compressed_path}")
        return compressed_path
        
    except Exception as e:
        logger.error(f"Error creating database backup: {str(e)}")
        return None

def upload_to_s3(file_path):
    """Upload backup file to S3"""
    try:
        s3_client = boto3.client(
            's3',
            aws_access_key_id="{{ vault_s3_access_key }}",
            aws_secret_access_key="{{ vault_s3_secret_key }}",
            region_name="{{ s3_region }}"
        )
        
        filename = os.path.basename(file_path)
        s3_key = f"{S3_PREFIX}/{filename}"
        
        logger.info(f"Uploading to S3: s3://{S3_BUCKET}/{s3_key}")
        
        # Upload with server-side encryption
        s3_client.upload_file(
            file_path,
            S3_BUCKET,
            s3_key,
            ExtraArgs={
                'ServerSideEncryption': 'AES256',
                'StorageClass': 'STANDARD_IA'
            }
        )
        
        logger.info(f"Successfully uploaded to S3: {s3_key}")
        return True
        
    except (ClientError, NoCredentialsError) as e:
        logger.error(f"S3 upload failed: {str(e)}")
        return False

def cleanup_local_files(file_path, keep_local_days={{ daily_retention // 2 }}):
    """Clean up local backup files older than specified days"""
    try:
        # Remove the current backup file after successful upload
        if os.path.exists(file_path):
            os.remove(file_path)
            logger.info(f"Removed local backup: {file_path}")
        
        # Clean up old local backups
        cutoff_time = datetime.datetime.now() - datetime.timedelta(days=keep_local_days)
        
        for filename in os.listdir(BACKUP_DIR):
            if filename.startswith(f"project88_{ENVIRONMENT}_db_") and filename.endswith('.sql.gz'):
                file_path = os.path.join(BACKUP_DIR, filename)
                file_time = datetime.datetime.fromtimestamp(os.path.getctime(file_path))
                
                if file_time < cutoff_time:
                    os.remove(file_path)
                    logger.info(f"Removed old local backup: {filename}")
                    
    except Exception as e:
        logger.error(f"Error during cleanup: {str(e)}")

def main():
    """Main backup process"""
    logger.info(f"Starting Project88 {ENVIRONMENT} database backup")
    
    # Ensure backup directory exists
    os.makedirs(BACKUP_DIR, exist_ok=True)
    
    # Create database backup
    backup_path = create_database_backup()
    if not backup_path:
        logger.error("Database backup failed")
        sys.exit(1)
    
    # Upload to S3
    if upload_to_s3(backup_path):
        logger.info("Backup completed successfully")
        cleanup_local_files(backup_path)
    else:
        logger.error("S3 upload failed, keeping local backup")
        sys.exit(1)

if __name__ == "__main__":
    main()